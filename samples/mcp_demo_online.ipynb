{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m42 packages\u001b[0m \u001b[2min 104ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m42 packages\u001b[0m \u001b[2min 0.05ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install textarena smithery anthropic httpx mcp --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk...\"\n",
    "# os.environ[\"E2B_API_KEY\"] = \"e2b...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textarena.core import Agent\n",
    "import textarena as ta\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "\n",
    "STANDARD_GAME_PROMPT = \"You are a competitive game player. Make sure you read the game instructions carefully, and always follow the required format.\"\n",
    "\n",
    "class AsyncAnthropicAgent(Agent):\n",
    "    \"\"\"Agent class using the Anthropic Claude API to generate responses asynchronously.\"\"\"\n",
    "    def __init__(self, model_name: str, system_prompt: Optional[str] = STANDARD_GAME_PROMPT, max_tokens: int = 1000, temperature: float = 0.9, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the Anthropic agent.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the Claude model (e.g., \"claude-3-5-sonnet-20241022\").\n",
    "            system_prompt (Optional[str]): The system prompt to use (default: STANDARD_GAME_PROMPT).\n",
    "            max_tokens (int): The maximum number of tokens to generate.\n",
    "            temperature (float): The temperature for randomness in response generation.\n",
    "            verbose (bool): If True, additional debug info will be printed.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.system_prompt = system_prompt\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        try:\n",
    "            import anthropic\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Anthropic package is required for AsyncAnthropicAgent. \"\n",
    "                \"Install it with: pip install anthropic\"\n",
    "            )\n",
    "            \n",
    "        self.client = anthropic.AsyncAnthropic()\n",
    "    \n",
    "    async def _make_request(self, observation: str) -> str:\n",
    "        \"\"\"Make a single API request to Anthropic and return the generated message.\"\"\"\n",
    "        response = await self.client.messages.create(\n",
    "            model=self.model_name,\n",
    "            max_tokens=self.max_tokens,\n",
    "            temperature=self.temperature,\n",
    "            system=self.system_prompt,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": observation}]}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return response.content[0].text.strip()\n",
    "    \n",
    "    async def _retry_request(self, observation: str, retries: int = 3, delay: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Attempt to make an API request with retries.\n",
    "\n",
    "        Args:\n",
    "            observation (str): The input to process.\n",
    "            retries (int): The number of attempts to try.\n",
    "            delay (int): Seconds to wait between attempts.\n",
    "\n",
    "        Raises:\n",
    "            Exception: The last exception caught if all retries fail.\n",
    "        \"\"\"\n",
    "        last_exception = None\n",
    "        for attempt in range(1, retries + 1):\n",
    "            try:\n",
    "                response = await self._make_request(observation)\n",
    "                if self.verbose:\n",
    "                    print(f\"\\nObservation: {observation}\\nResponse: {response}\")\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                last_exception = e\n",
    "                print(f\"Attempt {attempt} failed with error: {e}\")\n",
    "                if attempt < retries:\n",
    "                    await asyncio.sleep(delay)\n",
    "        raise last_exception\n",
    "    \n",
    "    async def __call__(self, observation: str) -> str:\n",
    "        \"\"\"\n",
    "        Process the observation using the Anthropic API and return the generated response.\n",
    "        \n",
    "        Args:\n",
    "            observation (str): The input string to process.\n",
    "        \n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        if not isinstance(observation, str):\n",
    "            raise ValueError(f\"Observation must be a string. Received type: {type(observation)}\")\n",
    "        return await self._retry_request(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textarena as ta\n",
    "import smithery\n",
    "import mcp\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "class MCPAgent(AsyncAnthropicAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.url = smithery.create_smithery_url(\n",
    "            \"wss://server.smithery.ai/@kwen1510/nltk-map/ws\", {\"e2bApiKey\": os.environ[\"E2B_API_KEY\"]}\n",
    "        )\n",
    "\n",
    "    async def _make_request(self, observation: str) -> str:\n",
    "        \"\"\"Make a single API request to Anthropic and return the generated message.\"\"\"\n",
    "        async with smithery.websocket_client(self.url) as streams:\n",
    "            async with mcp.client.session.ClientSession(*streams) as session:\n",
    "\n",
    "                try:\n",
    "                    tools_result = await session.list_tools()\n",
    "                    tools = tools_result.model_dump()[\"tools\"]\n",
    "\n",
    "                    tools = [\n",
    "                        {\"input_schema\": tool.pop(\"inputSchema\"), **tool}\n",
    "                        for tool in tools\n",
    "                        if \"inputSchema\" in tool\n",
    "                    ]\n",
    "\n",
    "                    print(\"Available tools:\", tools)\n",
    "\n",
    "                    final_response_text = \"\"\n",
    "                    is_tool_call_pending = True\n",
    "                    messages = [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [{\"type\": \"text\", \"text\": observation}],\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "                    # Loop to handle multiple tool calls in a conversation\n",
    "                    while is_tool_call_pending:\n",
    "                        response = await self.client.messages.create(\n",
    "                            model=self.model_name,\n",
    "                            max_tokens=self.max_tokens,\n",
    "                            temperature=self.temperature,\n",
    "                            system=self.system_prompt,\n",
    "                            messages=messages,\n",
    "                            tools=tools,\n",
    "                        )\n",
    "\n",
    "                        print(\"Response:\", response)\n",
    "\n",
    "                        # Check if there's a tool_use in the response\n",
    "                        is_tool_call_pending = False\n",
    "                        for content_block in response.content:\n",
    "                            if content_block.type == \"tool_use\":\n",
    "                                is_tool_call_pending = True\n",
    "\n",
    "                                tool_name = content_block.name\n",
    "                                tool_input = content_block.input\n",
    "                                tool_id = content_block.id\n",
    "\n",
    "                                print(f\"Tool called: {tool_name}\")\n",
    "                                print(f\"Tool input: {json.dumps(tool_input, indent=2)}\")\n",
    "\n",
    "                                # Execute the tool using MCP session\n",
    "                                try:\n",
    "                                    tool_result = await session.call_tool(\n",
    "                                        tool_name, tool_input\n",
    "                                    )\n",
    "\n",
    "                                    # Convert tool result to string format for Anthropic\n",
    "                                    # The content must be a string, not an object\n",
    "                                    tool_result_dict = tool_result.model_dump()\n",
    "                                except Exception as e:\n",
    "                                    if \"MCP error\" in str(e):\n",
    "                                        tool_result_dict = {\"error\": str(e)}\n",
    "\n",
    "                                result_str = json.dumps(tool_result_dict)\n",
    "                                print(f\"Tool result: {result_str}\")\n",
    "\n",
    "                                # Add tool call and result to messages\n",
    "                                messages.append(\n",
    "                                    {\n",
    "                                        \"role\": \"assistant\",\n",
    "                                        \"content\": [content_block.model_dump()],\n",
    "                                    }\n",
    "                                )\n",
    "\n",
    "                                # Add tool response to messages - content must be a string\n",
    "                                messages.append(\n",
    "                                    {\n",
    "                                        \"role\": \"user\",\n",
    "                                        \"content\": [\n",
    "                                            {\n",
    "                                                \"type\": \"tool_result\",\n",
    "                                                \"tool_use_id\": tool_id,\n",
    "                                                \"content\": result_str,  # Now it's a string\n",
    "                                            }\n",
    "                                        ],\n",
    "                                    }\n",
    "                                )\n",
    "                            elif content_block.type == \"text\":\n",
    "                                # Accumulate text responses\n",
    "                                final_response_text += content_block.text\n",
    "\n",
    "                            # Add Assistant Pre-Fill \n",
    "                            messages.append(\n",
    "                                {\n",
    "                                    \"role\": \"assistant\",\n",
    "                                    \"content\": [{\"type\": \"text\", \"text\": \"```json\\n{\\\"thinking\\\"}:\"}],\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                        # If no tool calls were made, we use the text response\n",
    "                        if not is_tool_call_pending and not final_response_text:\n",
    "                            final_response_text = response.content[0].text\n",
    "                            final_response_text = \"{\\\"thinking\\\"}\" + final_response_text\n",
    "                            final_response_text = json.loads(final_response_text)[\"action\"]\n",
    "\n",
    "                except Exception as e:\n",
    "\n",
    "                    print(f\"Error: {e}\")\n",
    "                    raise e\n",
    "\n",
    "            return final_response_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      7\u001b[39m env = ta.make_online(\n\u001b[32m      8\u001b[39m     env_id=[\u001b[33m\"\u001b[39m\u001b[33mSpellingBee-v0\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSimpleNegotiation-v0\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPoker-v0\u001b[39m\u001b[33m\"\u001b[39m], \n\u001b[32m      9\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33mTest 123456789\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     model_description=\u001b[33m\"\u001b[39m\u001b[33mTest 123456789\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     email=\u001b[33m\"\u001b[39m\u001b[33mTest 123456789\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m env = ta.wrappers.LLMObservationWrapper(env=env)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_players\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnest_asyncio\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/uv/archive-v0/D2QJW6r2-ihLsOiHREoB5/lib/python3.12/site-packages/textarena/core.py:389\u001b[39m, in \u001b[36mWrapper.reset\u001b[39m\u001b[34m(self, num_players, seed)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_players: \u001b[38;5;28mint\u001b[39m , seed: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_players\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_players\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/uv/archive-v0/D2QJW6r2-ihLsOiHREoB5/lib/python3.12/site-packages/textarena/api.py:464\u001b[39m, in \u001b[36mOnlineEnvWrapper.reset\u001b[39m\u001b[34m(self, num_players, seed)\u001b[39m\n\u001b[32m    461\u001b[39m     new_loop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43masync_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_players\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m new_loop:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:662\u001b[39m, in \u001b[36mBaseEventLoop.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    651\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[32m    652\u001b[39m \n\u001b[32m    653\u001b[39m \u001b[33;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    659\u001b[39m \u001b[33;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m new_task = \u001b[38;5;129;01mnot\u001b[39;00m futures.isfuture(future)\n\u001b[32m    665\u001b[39m future = tasks.ensure_future(future, loop=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:621\u001b[39m, in \u001b[36mBaseEventLoop._check_running\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_running():\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mThis event loop is already running\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    623\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    624\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mCannot run the event loop while another loop is running\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "import textarena as ta\n",
    "\n",
    "# Initialize agents\n",
    "agents = MCPAgent(model_name=\"claude-3-7-sonnet-20250219\"),\n",
    "\n",
    "# Initialize environment from subset and wrap it\n",
    "env = ta.make_online(\n",
    "    env_id=[\"SpellingBee-v0\", \"SimpleNegotiation-v0\", \"Poker-v0\"], \n",
    "    model_name=\"Test 123456789\",\n",
    "    model_description=\"Test 123456789\",\n",
    "    email=\"Test 123456789\"\n",
    ")\n",
    "env = ta.wrappers.LLMObservationWrapper(env=env)\n",
    "\n",
    "\n",
    "env.reset(num_players=1)\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    player_id, observation = env.get_observation()\n",
    "    action = asyncio.get_event_loop().run_until_complete(player_id)\n",
    "    done, info = env.step(action=action)\n",
    "    print(\"step complete\")\n",
    "    \n",
    "rewards = env.close()\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
